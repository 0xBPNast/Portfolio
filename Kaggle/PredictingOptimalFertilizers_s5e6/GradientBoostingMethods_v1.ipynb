{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65686a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb290858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class configs:\n",
    "\n",
    "    train_path = \"playground-series-s5e6/train.csv\"\n",
    "    test_path = \"playground-series-s5e6/test.csv\"\n",
    "    original_path = \"playground-series-s5e6/Fertilizer Prediction.csv\"\n",
    "    sample_sub_path = \"playground-series-s5e6/sample_submission.csv\"\n",
    "\n",
    "    target = \"Fertilizer Name\"\n",
    "    n_folds = 5\n",
    "    seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8f7bd",
   "metadata": {},
   "source": [
    "## 1. Data loading & Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26191b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (750000, 10)\n",
      "Test shape: (250000, 9)\n",
      "\n",
      "Train columns: ['id', 'Temperature', 'Humidity', 'Moisture', 'Soil Type', 'Crop Type', 'Nitrogen', 'Potassium', 'Phosphorous', 'Fertilizer Name']\n",
      "Test columns: ['id', 'Temperature', 'Humidity', 'Moisture', 'Soil Type', 'Crop Type', 'Nitrogen', 'Potassium', 'Phosphorous']\n",
      "\n",
      "Train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750000 entries, 0 to 749999\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   id               750000 non-null  int64 \n",
      " 1   Temperature      750000 non-null  int64 \n",
      " 2   Humidity         750000 non-null  int64 \n",
      " 3   Moisture         750000 non-null  int64 \n",
      " 4   Soil Type        750000 non-null  object\n",
      " 5   Crop Type        750000 non-null  object\n",
      " 6   Nitrogen         750000 non-null  int64 \n",
      " 7   Potassium        750000 non-null  int64 \n",
      " 8   Phosphorous      750000 non-null  int64 \n",
      " 9   Fertilizer Name  750000 non-null  object\n",
      "dtypes: int64(7), object(3)\n",
      "memory usage: 57.2+ MB\n",
      "\n",
      "Test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250000 entries, 0 to 249999\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   id           250000 non-null  int64 \n",
      " 1   Temperature  250000 non-null  int64 \n",
      " 2   Humidity     250000 non-null  int64 \n",
      " 3   Moisture     250000 non-null  int64 \n",
      " 4   Soil Type    250000 non-null  object\n",
      " 5   Crop Type    250000 non-null  object\n",
      " 6   Nitrogen     250000 non-null  int64 \n",
      " 7   Potassium    250000 non-null  int64 \n",
      " 8   Phosphorous  250000 non-null  int64 \n",
      "dtypes: int64(7), object(2)\n",
      "memory usage: 17.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "try:\n",
    "    train_df = pd.read_csv(configs.train_path)\n",
    "    test_df = pd.read_csv(configs.test_path)\n",
    "    sample_submission = pd.read_csv(configs.sample_sub_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Ensure 'train.csv', 'test.csv', and 'sample_submission.csv' are in the same directory.\")\n",
    "    # Fallback for Kaggle environment or if files are in parent directory\n",
    "    train_df = pd.read_csv('/kaggle/input/predicting-optimal-fertilizers/train.csv')\n",
    "    test_df = pd.read_csv('/kaggle/input/predicting-optimal-fertilizers/test.csv')\n",
    "    sample_submission = pd.read_csv('/kaggle/input/predicting-optimal-fertilizers/sample_submission.csv')\n",
    "\n",
    "if 'Temparature' in train_df.columns:\n",
    "    train_df = train_df.rename(columns={'Temparature': 'Temperature'})\n",
    "if 'Temparature' in test_df.columns:\n",
    "    test_df = test_df.rename(columns={'Temparature': 'Temperature'})\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"\\nTrain columns:\", train_df.columns.tolist())\n",
    "print(\"Test columns:\", test_df.columns.tolist())\n",
    "print(\"\\nTrain info:\")\n",
    "train_df.info()\n",
    "print(\"\\nTest info:\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60877b",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c06f2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features after engineering:\n",
      "['id', 'Temperature', 'Humidity', 'Moisture', 'Soil Type', 'Crop Type', 'Nitrogen', 'Potassium', 'Phosphorous', 'Fertilizer Name', 'N_P_ratio', 'N_K_ratio', 'P_K_ratio', 'N_P_K_sum', 'N_P_K_prod', 'Soil_Crop_Type', 'Temp_Hum_Index', 'Temp_Moist_Index', 'Hum_Moist_Index']\n"
     ]
    }
   ],
   "source": [
    "def feature_engineer(df):\n",
    "    # Nutrient Ratios (highly impactful)\n",
    "    df['N_P_ratio'] = df['Nitrogen'] / (df['Phosphorous'] + 1e-6) # Add epsilon to avoid division by zero\n",
    "    df['N_K_ratio'] = df['Nitrogen'] / (df['Potassium'] + 1e-6)\n",
    "    df['P_K_ratio'] = df['Phosphorous'] / (df['Potassium'] + 1e-6)\n",
    "    df['N_P_K_sum'] = df['Nitrogen'] + df['Phosphorous'] + df['Potassium']\n",
    "    df['N_P_K_prod'] = df['Nitrogen'] * df['Phosphorous'] * df['Potassium']\n",
    "\n",
    "    # Interaction between Soil Type and Crop Type\n",
    "    df['Soil_Crop_Type'] = df['Soil Type'] + '_' + df['Crop Type']\n",
    "\n",
    "    # Climate Index\n",
    "    df['Temp_Hum_Index'] = df['Temperature'] * df['Humidity'] / 100\n",
    "    df['Temp_Moist_Index'] = df['Temperature'] * df['Moisture'] / 100\n",
    "    df['Hum_Moist_Index'] = df['Humidity'] * df['Moisture'] / 100\n",
    "\n",
    "    # Polynomial features for nutrients (consider if models need more non-linearity)\n",
    "    # df['N_sq'] = df['Nitrogen']**2\n",
    "    # df['P_sq'] = df['Phosphorous']**2\n",
    "    # df['K_sq'] = df['Potassium']**2\n",
    "\n",
    "    # Binning (e.g., for nutrient levels - Low/Med/High) - might be useful for tree splits\n",
    "    # This requires defining sensible bins based on EDA\n",
    "    # bins_N = [0, 10, 30, np.inf]\n",
    "    # labels_N = ['N_Low', 'N_Medium', 'N_High']\n",
    "    # df['Nitrogen_bin'] = pd.cut(df['Nitrogen'], bins=bins_N, labels=labels_N, right=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = feature_engineer(train_df)\n",
    "test_df = feature_engineer(test_df)\n",
    "\n",
    "print(\"\\nFeatures after engineering:\")\n",
    "print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d3839",
   "metadata": {},
   "source": [
    "## 3. Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7f20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of target classes: 7\n",
      "Target classes mapping: {'10-26-26': 0, '14-35-14': 1, '17-17-17': 2, '20-20': 3, '28-28': 4, 'DAP': 5, 'Urea': 6}\n"
     ]
    }
   ],
   "source": [
    "# Label Encode the target variable\n",
    "le = LabelEncoder()\n",
    "train_df['Fertilizer_Encoded'] = le.fit_transform(train_df['Fertilizer Name'])\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"\\nNumber of target classes: {num_classes}\")\n",
    "print(\"Target classes mapping:\", dict(zip(le.classes_, range(num_classes))))\n",
    "\n",
    "# Identify categorical features (including the engineered one)\n",
    "categorical_features = ['Soil Type', 'Crop Type', 'Soil_Crop_Type'] # Add other binned features if created\n",
    "\n",
    "# Convert categorical features to 'category' dtype for LightGBM/CatBoost\n",
    "for col in categorical_features:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')\n",
    "\n",
    "# For XGBoost (if native support isn't fully utilized, or for older versions):\n",
    "# One-Hot Encoding (for models that don't support native categoricals or for specific cases)\n",
    "# However, for LightGBM/CatBoost, native handling is generally preferred.\n",
    "# train_df = pd.get_dummies(train_df, columns=categorical_features, drop_first=True)\n",
    "# test_df = pd.get_dummies(test_df, columns=categorical_features, drop_first=True)\n",
    "# Align columns after one-hot encoding\n",
    "# train_cols = set(train_df.columns)\n",
    "# test_cols = set(test_df.columns)\n",
    "# common_cols = list(train_cols.intersection(test_cols))\n",
    "# train_df = train_df[common_cols + ['Fertilizer_Encoded', 'Fertilizer Name']] # Keep target\n",
    "# test_df = test_df[common_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722670d",
   "metadata": {},
   "source": [
    "## 4. MAP@3 Metric Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d21da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(predictions, actuals, k=3):\n",
    "    \"\"\"\n",
    "    Calculates Mean Average Precision @ k.\n",
    "    predictions: List of lists, where each inner list contains predicted labels sorted by confidence.\n",
    "    actuals: List of actual labels.\n",
    "    \"\"\"\n",
    "    if len(predictions) != len(actuals):\n",
    "        raise ValueError(\"Predictions and actuals must have the same length.\")\n",
    "\n",
    "    total_precision = 0.0\n",
    "    for i in range(len(predictions)):\n",
    "        predicted_labels = predictions[i][:k] # Only consider top K\n",
    "        actual_label = actuals[i]\n",
    "\n",
    "        ap = 0.0\n",
    "        num_hits = 0\n",
    "        for j, pred_label in enumerate(predicted_labels):\n",
    "            if pred_label == actual_label:\n",
    "                num_hits += 1\n",
    "                ap += num_hits / (j + 1)\n",
    "                break # Only count the first correct prediction for Average Precision\n",
    "\n",
    "        total_precision += ap\n",
    "\n",
    "    return total_precision / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db6c49",
   "metadata": {},
   "source": [
    "## 5. Model Training (Gradient Boosting Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90294b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting K-Fold Training...\n",
      "--- Fold 1/5 ---\n",
      "Training LightGBM...\n",
      "Training XGBoost...\n",
      "Training CatBoost...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    100\u001b[39m cat_params = {\n\u001b[32m    101\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mMultiClass\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    102\u001b[39m     \u001b[33m'\u001b[39m\u001b[33miterations\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m2000\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcat_features\u001b[39m\u001b[33m'\u001b[39m: cat_categorical_features_indices,\n\u001b[32m    113\u001b[39m }\n\u001b[32m    114\u001b[39m cat_model = cb.CatBoostClassifier(**cat_params)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43mcat_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m              \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m              \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m oof_preds_cat[val_idx] = cat_model.predict_proba(X_val)\n\u001b[32m    119\u001b[39m test_preds_cat += cat_model.predict_proba(X_test) / NFOLDS\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brad\\anaconda3\\envs\\myenv\\Lib\\site-packages\\catboost\\core.py:5245\u001b[39m, in \u001b[36mCatBoostClassifier.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5243\u001b[39m     CatBoostClassifier._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5246\u001b[39m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5247\u001b[39m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brad\\anaconda3\\envs\\myenv\\Lib\\site-packages\\catboost\\core.py:2410\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2407\u001b[39m allow_clear_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mallow_clear_pool\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[33m'\u001b[39m\u001b[33mTraining plots\u001b[39m\u001b[33m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m.get_params())]):\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_sets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[32m   2419\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._object._get_loss_function_name()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brad\\anaconda3\\envs\\myenv\\Lib\\site-packages\\catboost\\core.py:1790\u001b[39m, in \u001b[36m_CatBoostBase._train\u001b[39m\u001b[34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_trained_model_attributes()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5023\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5072\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "FEATURES = [col for col in train_df.columns if col not in ['id', 'Fertilizer Name', 'Fertilizer_Encoded']]\n",
    "TARGET = 'Fertilizer_Encoded'\n",
    "\n",
    "X = train_df[FEATURES]\n",
    "y = train_df[TARGET]\n",
    "X_test = test_df[FEATURES]\n",
    "\n",
    "# Ensure categorical features are correctly recognized by LightGBM/CatBoost\n",
    "lgbm_categorical_features = [col for col in FEATURES if X[col].dtype.name == 'category']\n",
    "cat_categorical_features_indices = [X.columns.get_loc(col) for col in lgbm_categorical_features]\n",
    "\n",
    "NFOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds_lgb = np.zeros((len(X), num_classes))\n",
    "oof_preds_xgb = np.zeros((len(X), num_classes))\n",
    "oof_preds_cat = np.zeros((len(X), num_classes))\n",
    "\n",
    "test_preds_lgb = np.zeros((len(X_test), num_classes))\n",
    "test_preds_xgb = np.zeros((len(X_test), num_classes))\n",
    "test_preds_cat = np.zeros((len(X_test), num_classes))\n",
    "\n",
    "models = {\n",
    "    'lgb': [],\n",
    "    'xgb': [],\n",
    "    'cat': []\n",
    "}\n",
    "\n",
    "print(\"\\nStarting K-Fold Training...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"--- Fold {fold+1}/{NFOLDS} ---\")\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    # LightGBM\n",
    "    print(\"Training LightGBM...\")\n",
    "    lgb_params = {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': num_classes,\n",
    "        'metric': 'multi_logloss', # MAP@3 not directly optimized by default in LGBM\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 2000, # Increased for potential early stopping\n",
    "        'learning_rate': 0.02,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'seed': 42 + fold,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1, # Suppress verbose output\n",
    "        'colsample_bytree': 0.7,\n",
    "        'subsample': 0.7,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'min_child_samples': 20,\n",
    "        'early_stopping_round': 100 # Add early stopping\n",
    "    }\n",
    "    lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "    lgb_model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  eval_metric='multi_logloss', # or use 'map' if custom metric is implemented in LGBM\n",
    "                  callbacks=[lgb.log_evaluation(period=0)], # Suppress period logging\n",
    "                  categorical_feature=lgbm_categorical_features)\n",
    "    oof_preds_lgb[val_idx] = lgb_model.predict_proba(X_val)\n",
    "    test_preds_lgb += lgb_model.predict_proba(X_test) / NFOLDS\n",
    "    models['lgb'].append(lgb_model)\n",
    "    gc.collect()\n",
    "\n",
    "    # XGBoost\n",
    "    print(\"Training XGBoost...\")\n",
    "    xgb_params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': num_classes,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'tree_method': 'hist', # Still use 'hist' for general efficiency\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': 0.02,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'gamma': 0.1,\n",
    "        'lambda': 0.1,\n",
    "        'alpha': 0.1,\n",
    "        'random_state': 42 + fold,\n",
    "        'n_jobs': -1,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'enable_categorical': True, # <--- ADD THIS LINE\n",
    "    }\n",
    "    xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "    xgb_model.fit(X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=False)\n",
    "    oof_preds_xgb[val_idx] = xgb_model.predict_proba(X_val)\n",
    "    test_preds_xgb += xgb_model.predict_proba(X_test) / NFOLDS\n",
    "    models['xgb'].append(xgb_model)\n",
    "    gc.collect()\n",
    "\n",
    "    # CatBoost\n",
    "    print(\"Training CatBoost...\")\n",
    "    cat_params = {\n",
    "        'objective': 'MultiClass',\n",
    "        'iterations': 2000,\n",
    "        'learning_rate': 0.02,\n",
    "        'depth': 6,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'loss_function': 'MultiClass',\n",
    "        'eval_metric': 'MultiClass', # MAP@3 not directly optimized by default in CatBoost\n",
    "        'random_seed': 42 + fold,\n",
    "        'verbose': 0, # Suppress verbose output\n",
    "        'early_stopping_rounds': 100, # Add early stopping\n",
    "        'allow_writing_files': False, # Important for Kaggle notebooks to prevent I/O errors\n",
    "        'cat_features': cat_categorical_features_indices,\n",
    "    }\n",
    "    cat_model = cb.CatBoostClassifier(**cat_params)\n",
    "    cat_model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  verbose=False)\n",
    "    oof_preds_cat[val_idx] = cat_model.predict_proba(X_val)\n",
    "    test_preds_cat += cat_model.predict_proba(X_test) / NFOLDS\n",
    "    models['cat'].append(cat_model)\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\nTraining Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62655814",
   "metadata": {},
   "source": [
    "## 6. Ensemble and MAP@3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate OOF predictions for each model type\n",
    "oof_actuals = y.tolist()\n",
    "\n",
    "oof_lgb_ranked_preds = []\n",
    "for row in oof_preds_lgb:\n",
    "    sorted_indices = np.argsort(row)[::-1]\n",
    "    oof_lgb_ranked_preds.append(le.inverse_transform(sorted_indices).tolist())\n",
    "map3_lgb = map_at_k(oof_lgb_ranked_preds, oof_actuals, k=3)\n",
    "print(f\"LightGBM OOF MAP@3: {map3_lgb:.5f}\")\n",
    "\n",
    "oof_xgb_ranked_preds = []\n",
    "for row in oof_preds_xgb:\n",
    "    sorted_indices = np.argsort(row)[::-1]\n",
    "    oof_xgb_ranked_preds.append(le.inverse_transform(sorted_indices).tolist())\n",
    "map3_xgb = map_at_k(oof_xgb_ranked_preds, oof_actuals, k=3)\n",
    "print(f\"XGBoost OOF MAP@3: {map3_xgb:.5f}\")\n",
    "\n",
    "oof_cat_ranked_preds = []\n",
    "for row in oof_preds_cat:\n",
    "    sorted_indices = np.argsort(row)[::-1]\n",
    "    oof_cat_ranked_preds.append(le.inverse_transform(sorted_indices).tolist())\n",
    "map3_cat = map_at_k(oof_cat_ranked_preds, oof_actuals, k=3)\n",
    "print(f\"CatBoost OOF MAP@3: {map3_cat:.5f}\")\n",
    "\n",
    "# Simple Averaging Ensemble of OOF predictions\n",
    "oof_ensemble_preds = (oof_preds_lgb + oof_preds_xgb + oof_preds_cat) / 3\n",
    "\n",
    "oof_ensemble_ranked_preds = []\n",
    "for row in oof_ensemble_preds:\n",
    "    sorted_indices = np.argsort(row)[::-1]\n",
    "    oof_ensemble_ranked_preds.append(le.inverse_transform(sorted_indices).tolist())\n",
    "map3_ensemble = map_at_k(oof_ensemble_ranked_preds, oof_actuals, k=3)\n",
    "print(f\"Ensemble (Average) OOF MAP@3: {map3_ensemble:.5f}\")\n",
    "\n",
    "# More advanced ensembling: Stacking\n",
    "# You could train a meta-model (e.g., Logistic Regression) on oof_preds_lgb, oof_preds_xgb, oof_preds_cat\n",
    "# to predict the target. This often yields better results.\n",
    "# Example:\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# meta_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=42)\n",
    "# meta_model.fit(oof_ensemble_preds, y) # Train meta-model on combined OOF predictions\n",
    "# Then use meta_model.predict_proba on the combined test_preds to get final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6cbd0",
   "metadata": {},
   "source": [
    "## 7. Submission File Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Averaging Ensemble of test predictions\n",
    "final_test_preds_proba = (test_preds_lgb + test_preds_xgb + test_preds_cat) / 3\n",
    "\n",
    "# For each row, get the top 3 predicted fertilizer names\n",
    "predictions_for_submission = []\n",
    "for proba_row in final_test_preds_proba:\n",
    "    # Get indices of the top 3 probabilities\n",
    "    top_3_indices = np.argsort(proba_row)[::-1][:3]\n",
    "    # Convert indices back to original fertilizer names\n",
    "    top_3_fertilizers = le.inverse_transform(top_3_indices)\n",
    "    predictions_for_submission.append(\" \".join(top_3_fertilizers))\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'Fertilizer Name': predictions_for_submission})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file created: submission.csv\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
