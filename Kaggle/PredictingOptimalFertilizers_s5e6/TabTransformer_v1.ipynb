{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6770f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, OrdinalEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87b040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class configs:\n",
    "\n",
    "    train_path = \"playground-series-s5e6/train.csv\"\n",
    "    test_path = \"playground-series-s5e6/test.csv\"\n",
    "    original_path = \"playground-series-s5e6/Fertilizer Prediction.csv\"\n",
    "    sample_sub_path = \"playground-series-s5e6/sample_submission.csv\"\n",
    "\n",
    "    target = \"Fertilizer Name\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    early_stopping_patience = 29\n",
    "    max_expochs = 30\n",
    "    batch_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785a2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PARAMS = {\n",
    "    'embed_dim': 128,\n",
    "    'num_heads': 8,\n",
    "    'num_transformer_layers': 5,\n",
    "    'ff_hidden_dim': 512,\n",
    "    'dropout_rate': 0.2,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f150fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set all seeds immediately for reproducibility\n",
    "np.random.seed(configs.seed)\n",
    "python_random.seed(configs.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(configs.seed)\n",
    "torch.manual_seed(configs.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(configs.seed)\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31e253c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train data loaded. Shape: (750000, 10)\n",
      "Additional Train data loaded. Shape: (100000, 9)\n",
      "Test data loaded. Shape: (250000, 9)\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data ---\n",
    "df_train_original  = pd.read_csv(configs.train_path)\n",
    "df_train_additional = pd.read_csv(configs.original_path)\n",
    "df_test = pd.read_csv(configs.test_path)\n",
    "\n",
    "print(\"Original Train data loaded. Shape:\", df_train_original.shape)\n",
    "print(\"Additional Train data loaded. Shape:\", df_train_additional.shape)\n",
    "print(\"Test data loaded. Shape:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbc09417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define features and target ---\n",
    "TARGET = 'Fertilizer Name'\n",
    "ID_COL = 'id'\n",
    "\n",
    "# Separate features (X) and target (y) for original and additional datasets\n",
    "X_original = df_train_original.drop([ID_COL, TARGET], axis=1).copy()\n",
    "y_original = df_train_original[TARGET].copy()\n",
    "\n",
    "X_additional = df_train_additional.drop(TARGET, axis=1).copy()\n",
    "y_additional = df_train_additional[TARGET].copy()\n",
    "\n",
    "X_test = df_test.drop(ID_COL, axis=1).copy()\n",
    "test_ids = df_test[ID_COL].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "140d46e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing Feature Engineering...\n",
      "Feature Engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Feature Engineering ---\n",
    "print(\"\\nPerforming Feature Engineering...\")\n",
    "\n",
    "original_numerical_cols = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n",
    "\n",
    "# Function to apply feature engineering for base features\n",
    "def apply_base_feature_engineering(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Temp_Humidity_Interaction'] = df_copy['Temparature'] * df_copy['Humidity']\n",
    "    df_copy['N_P_Ratio'] = df_copy['Nitrogen'] / (df_copy['Phosphorous'].replace(0, 1e-6))\n",
    "    df_copy['K_P_Ratio'] = df_copy['Potassium'] / (df_copy['Phosphorous'].replace(0, 1e-6))\n",
    "    df_copy['Soil_Crop_Combination'] = df_copy['Soil Type'].astype(str) + '_' + df_copy['Crop Type'].astype(str)\n",
    "    df_copy['Total_Nutrients'] = df_copy['Nitrogen'] + df_copy['Potassium'] + df_copy['Phosphorous']\n",
    "\n",
    "    df_copy['Temp_Nitrogen_Ratio'] = df_copy['Temparature'] / (df_copy['Nitrogen'].replace(0, 1e-6))\n",
    "    df_copy['Humidity_Phosphorous_Ratio'] = df_copy['Humidity'] / (df_copy['Phosphorous'].replace(0, 1e-6))\n",
    "    df_copy['Moisture_Potassium_Ratio'] = df_copy['Moisture'] / (df_copy['Potassium'].replace(0, 1e-6))\n",
    "    \n",
    "    df_copy['N_P_K_Ratio_Combined'] = df_copy['Nitrogen'] / (df_copy['Potassium'].replace(0, 1e-6) + df_copy['Phosphorous'].replace(0, 1e-6) + 1e-6)\n",
    "\n",
    "    df_copy['Nutrient_Mean'] = df_copy[['Nitrogen', 'Potassium', 'Phosphorous']].mean(axis=1)\n",
    "    df_copy['Nutrient_Imbalance_N'] = (df_copy['Nitrogen'] - df_copy['Nutrient_Mean']).abs()\n",
    "    df_copy['Nutrient_Imbalance_P'] = (df_copy['Phosphorous'] - df_copy['Nutrient_Mean']).abs()\n",
    "    df_copy['Nutrient_Imbalance_K'] = (df_copy['Potassium'] - df_copy['Nutrient_Mean']).abs()\n",
    "    \n",
    "    df_copy = df_copy.drop('Nutrient_Mean', axis=1)\n",
    "\n",
    "    # Binning numerical features (as strings for categorical handling)\n",
    "    for col in original_numerical_cols:\n",
    "        df_copy[f'{col}_Binned'] = df_copy[col].astype(str)\n",
    "    return df_copy\n",
    "\n",
    "# Apply base FE to all datasets\n",
    "X_original_temp = apply_base_feature_engineering(X_original)\n",
    "X_additional_temp = apply_base_feature_engineering(X_additional)\n",
    "X_test_temp = apply_base_feature_engineering(X_test)\n",
    "\n",
    "print(\"Feature Engineering complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3963a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define feature lists after base FE ---\n",
    "new_numerical_features = [\n",
    "    'Temp_Humidity_Interaction', 'N_P_Ratio', 'K_P_Ratio', 'Total_Nutrients',\n",
    "    'Temp_Nitrogen_Ratio', 'Humidity_Phosphorous_Ratio', 'Moisture_Potassium_Ratio',\n",
    "    'N_P_K_Ratio_Combined', 'Nutrient_Imbalance_N', 'Nutrient_Imbalance_P', 'Nutrient_Imbalance_K'\n",
    "]\n",
    "\n",
    "base_categorical_features = ['Soil Type', 'Crop Type', 'Soil_Crop_Combination']\n",
    "base_categorical_features.extend([f'{col}_Binned' for col in original_numerical_cols])\n",
    "\n",
    "\n",
    "# Polynomial Features (fit on original numerical columns, transform all)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Fit on original numerical features from original_numerical_cols\n",
    "X_original_poly_transformed = poly.fit_transform(X_original_temp[original_numerical_cols])\n",
    "X_additional_poly_transformed = poly.transform(X_additional_temp[original_numerical_cols])\n",
    "X_test_poly_transformed = poly.transform(X_test_temp[original_numerical_cols])\n",
    "\n",
    "poly_feature_names = poly.get_feature_names_out(original_numerical_cols)\n",
    "\n",
    "# Create separate DataFrames for polynomial features\n",
    "df_poly_original = pd.DataFrame(X_original_poly_transformed, columns=poly_feature_names, index=X_original_temp.index)\n",
    "df_poly_additional = pd.DataFrame(X_additional_poly_transformed, columns=poly_feature_names, index=X_additional_temp.index)\n",
    "df_poly_test = pd.DataFrame(X_test_poly_transformed, columns=poly_feature_names, index=X_test_temp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a3c5948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature List Verification (POST-CONSTRUCTION) ---\n",
      "Number of final numerical features: 38\n",
      "Number of final categorical features: 9\n",
      "Total features (numerical + categorical): 47\n",
      "X_original_fe.shape after final construction: (750000, 47)\n",
      "X_original_fe numerical columns (derived): 38\n",
      "X_original_fe categorical columns (derived): 9\n",
      "--- End Feature List Verification (POST-CONSTRUCTION) ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Constructing Final DataFrames with correct column sets ---\n",
    "# Final numerical features will be the `new_numerical_features` + all `poly_feature_names`\n",
    "final_numerical_features = sorted(list(set(new_numerical_features) | set(poly_feature_names)))\n",
    "\n",
    "# Final categorical features are simply the base_categorical_features\n",
    "final_categorical_features = sorted(list(set(base_categorical_features)))\n",
    "\n",
    "# Ensure no overlap\n",
    "assert set(final_numerical_features).isdisjoint(set(final_categorical_features)), \\\n",
    "    \"FATAL: Overlap detected between numerical and categorical features!\"\n",
    "\n",
    "\n",
    "# Build the final DataFrames by concatenating the exact numerical and categorical parts\n",
    "X_original_fe = pd.concat([\n",
    "    X_original_temp[new_numerical_features],\n",
    "    df_poly_original,\n",
    "    X_original_temp[final_categorical_features]\n",
    "], axis=1)[final_numerical_features + final_categorical_features].copy()\n",
    "\n",
    "X_additional_fe = pd.concat([\n",
    "    X_additional_temp[new_numerical_features],\n",
    "    df_poly_additional,\n",
    "    X_additional_temp[final_categorical_features]\n",
    "], axis=1)[final_numerical_features + final_categorical_features].copy()\n",
    "\n",
    "X_test_fe = pd.concat([\n",
    "    X_test_temp[new_numerical_features],\n",
    "    df_poly_test,\n",
    "    X_test_temp[final_categorical_features]\n",
    "], axis=1)[final_numerical_features + final_categorical_features].copy()\n",
    "\n",
    "\n",
    "print(\"\\n--- Feature List Verification (POST-CONSTRUCTION) ---\")\n",
    "print(f\"Number of final numerical features: {len(final_numerical_features)}\")\n",
    "print(f\"Number of final categorical features: {len(final_categorical_features)}\")\n",
    "print(f\"Total features (numerical + categorical): {len(final_numerical_features) + len(final_categorical_features)}\")\n",
    "print(f\"X_original_fe.shape after final construction: {X_original_fe.shape}\")\n",
    "print(f\"X_original_fe numerical columns (derived): {X_original_fe[final_numerical_features].shape[1]}\")\n",
    "print(f\"X_original_fe categorical columns (derived): {X_original_fe[final_categorical_features].shape[1]}\")\n",
    "assert len(final_numerical_features) == X_original_fe[final_numerical_features].shape[1], \"Numerical feature count mismatch!\"\n",
    "assert len(final_categorical_features) == X_original_fe[final_categorical_features].shape[1], \"Categorical feature count mismatch!\"\n",
    "assert X_original_fe.shape[1] == len(final_numerical_features) + len(final_categorical_features), \"Total column count mismatch!\"\n",
    "print(\"--- End Feature List Verification (POST-CONSTRUCTION) ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "601a79cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing features for TabTransformer...\n",
      "Feature preprocessing complete.\n",
      "Processed X_original_fe shape: (750000, 47)\n",
      "Processed X_additional_fe shape: (100000, 47)\n",
      "Processed X_test_fe shape: (250000, 47)\n",
      "Number of final numerical features: 38\n",
      "Number of final categorical features: 9\n"
     ]
    }
   ],
   "source": [
    "# --- Categorical & Numerical Preprocessing for TabTransformer ---\n",
    "print(\"\\nPreprocessing features for TabTransformer...\")\n",
    "\n",
    "# Ordinal Encoding for Categorical Features\n",
    "categorical_dims = {} # Store num unique categories for embedding layers\n",
    "for col in final_categorical_features:\n",
    "    combined_series = pd.concat([\n",
    "        X_original_fe[col].astype(str),\n",
    "        X_additional_fe[col].astype(str),\n",
    "        X_test_fe[col].astype(str)\n",
    "    ], axis=0)\n",
    "    \n",
    "    # Get all unique categories from the combined data\n",
    "    all_unique_cats_for_col = sorted(combined_series.astype('category').cat.categories.tolist())\n",
    "    \n",
    "    oenc = OrdinalEncoder(dtype=np.int64, categories=[all_unique_cats_for_col]) # Use int64 for embeddings, specify categories\n",
    "    \n",
    "    # Transform all dataframes. This will give 0-based indices.\n",
    "    X_original_fe[col] = oenc.fit_transform(X_original_fe[col].astype(str).values.reshape(-1, 1)).flatten()\n",
    "    X_additional_fe[col] = oenc.transform(X_additional_fe[col].astype(str).values.reshape(-1, 1)).flatten()\n",
    "    X_test_fe[col] = oenc.transform(X_test_fe[col].astype(str).values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    X_original_fe[col] = X_original_fe[col] + 1\n",
    "    X_additional_fe[col] = X_additional_fe[col] + 1\n",
    "    X_test_fe[col] = X_test_fe[col] + 1\n",
    "\n",
    "    categorical_dims[col] = len(all_unique_cats_for_col) + 1\n",
    "\n",
    "# Numerical Scaling\n",
    "scaler = StandardScaler()\n",
    "# Fit scaler on combined original and additional training data\n",
    "scaler.fit(pd.concat([X_original_fe[final_numerical_features], X_additional_fe[final_numerical_features]], axis=0))\n",
    "\n",
    "X_original_fe[final_numerical_features] = scaler.transform(X_original_fe[final_numerical_features])\n",
    "X_additional_fe[final_numerical_features] = scaler.transform(X_additional_fe[final_numerical_features])\n",
    "X_test_fe[final_numerical_features] = scaler.transform(X_test_fe[final_numerical_features])\n",
    "\n",
    "print(\"Feature preprocessing complete.\")\n",
    "print(\"Processed X_original_fe shape:\", X_original_fe.shape)\n",
    "print(\"Processed X_additional_fe shape:\", X_additional_fe.shape)\n",
    "print(\"Processed X_test_fe shape:\", X_test_fe.shape)\n",
    "print(f\"Number of final numerical features: {len(final_numerical_features)}\")\n",
    "print(f\"Number of final categorical features: {len(final_categorical_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ef21f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target encoding complete. Fertilizer classes (order): ['10-26-26' '14-35-14' '17-17-17' '20-20' '28-28' 'DAP' 'Urea']\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "# --- Target Encoding ---\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded_all_train = label_encoder.fit_transform(pd.concat([y_original, y_additional]).values)\n",
    "y_original_encoded = label_encoder.transform(y_original.values)\n",
    "y_additional_encoded = label_encoder.transform(y_additional.values) \n",
    "\n",
    "fertilizer_classes = label_encoder.classes_\n",
    "NUM_CLASSES = len(fertilizer_classes)\n",
    "print(\"\\nTarget encoding complete. Fertilizer classes (order):\", fertilizer_classes)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6fe6f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking target class distribution...\n",
      "Normalized class frequencies (encoded):\n",
      "  Class 10-26-26 (0): 0.1509\n",
      "  Class 14-35-14 (1): 0.1517\n",
      "  Class 17-17-17 (2): 0.1490\n",
      "  Class 20-20 (3): 0.1471\n",
      "  Class 28-28 (4): 0.1475\n",
      "  Class DAP (5): 0.1283\n",
      "  Class Urea (6): 0.1255\n",
      "\n",
      "Class distribution appears relatively balanced. No class weights applied.\n"
     ]
    }
   ],
   "source": [
    "# --- Check and Apply Class Weights for Imbalance ---\n",
    "print(\"\\nChecking target class distribution...\")\n",
    "class_counts = pd.Series(y_encoded_all_train).value_counts(normalize=True).sort_index()\n",
    "print(\"Normalized class frequencies (encoded):\")\n",
    "for i, count in enumerate(class_counts):\n",
    "    print(f\"  Class {label_encoder.inverse_transform([i])[0]} ({i}): {count:.4f}\")\n",
    "\n",
    "\n",
    "min_freq = class_counts.min()\n",
    "max_freq = class_counts.max()\n",
    "\n",
    "if max_freq / min_freq > 1.5:\n",
    "    print(\"\\nClass imbalance detected. Calculating balanced class weights...\")\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_encoded_all_train), y=y_encoded_all_train)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
    "    print(\"Class weights (encoded order):\", class_weights)\n",
    "else:\n",
    "    print(\"\\nClass distribution appears relatively balanced. No class weights applied.\")\n",
    "    class_weights_tensor = None # No weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56e9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAP@3 Calculation Functions ---\n",
    "def apk(actual, predicted, k=3):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=3):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47a5dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TabTransformer Model Definition (PyTorch) ---\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, numerical_features_count, categorical_dims_dict, \n",
    "                 embed_dim=32, num_heads=4, num_transformer_layers=3, \n",
    "                 ff_hidden_dim=128, dropout_rate=0.1, num_classes=7):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.numerical_dim = numerical_features_count\n",
    "        self.categorical_features_count = len(categorical_dims_dict)\n",
    "        \n",
    "        # Categorical Embeddings\n",
    "        # Ensure order is consistent by iterating through sorted keys when creating embeddings\n",
    "        self.cat_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_categories, embed_dim) for col, num_categories in sorted(categorical_dims_dict.items())\n",
    "        ])\n",
    "        \n",
    "        # Linear layer for numerical features\n",
    "        self.numerical_proj = nn.Linear(self.numerical_dim, embed_dim)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=ff_hidden_dim, \n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=num_transformer_layers)\n",
    "        \n",
    "        self.mlp_input_dim = embed_dim * (1 + self.categorical_features_count) \n",
    "        \n",
    "        # --- Deeper MLP Head with Batch Normalization ---\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.mlp_input_dim, ff_hidden_dim),\n",
    "            nn.BatchNorm1d(ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(ff_hidden_dim, ff_hidden_dim // 2),\n",
    "            nn.BatchNorm1d(ff_hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(ff_hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_numerical, x_categorical):\n",
    " \n",
    "        cat_embeddings = [emb(x_categorical[:, i]) for i, emb in enumerate(self.cat_embeddings)]\n",
    "        cat_embeddings = torch.stack(cat_embeddings, dim=1)\n",
    "        \n",
    "        # Process numerical features\n",
    "        num_proj = self.numerical_proj(x_numerical).unsqueeze(1)\n",
    "        \n",
    "        # Concatenate for transformer input\n",
    "        transformer_input = torch.cat([num_proj, cat_embeddings], dim=1)\n",
    "        \n",
    "        # Pass through transformer\n",
    "        transformer_output = self.transformer_encoder(transformer_input)\n",
    "        \n",
    "        # Flatten for MLP\n",
    "        flattened_output = transformer_output.view(transformer_output.size(0), -1) \n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.mlp_head(flattened_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0be199ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training and Evaluation Function ---\n",
    "def train_and_evaluate_model(model, train_loader, val_loader, optimizer, criterion, \n",
    "                             epochs, device, early_stopping_patience, fertilizer_classes, \n",
    "                             y_val_original_labels):\n",
    "    \n",
    "    best_map3 = -1.0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    print(f\"Training on {len(train_loader.dataset)} samples, validating on {len(val_loader.dataset)} samples.\")\n",
    "\n",
    "    # --- Advanced Learning Rate Scheduler ---\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6, verbose=True)\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for num_batch, cat_batch, labels_batch in train_loader:\n",
    "            num_batch, cat_batch, labels_batch = num_batch.to(device), cat_batch.to(device), labels_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # --- Mixed Precision Training ---\n",
    "            with autocast():\n",
    "                outputs = model(num_batch, cat_batch)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_preds_proba = []\n",
    "        val_true_labels_encoded = []\n",
    "        with torch.no_grad():\n",
    "            for num_batch, cat_batch, labels_batch in val_loader:\n",
    "                num_batch, cat_batch, labels_batch = num_batch.to(device), cat_batch.to(device), labels_batch.to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(num_batch, cat_batch)\n",
    "                \n",
    "                val_preds_proba.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "                val_true_labels_encoded.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        val_preds_proba = np.array(val_preds_proba)\n",
    "        \n",
    "        val_ranked_labels = []\n",
    "        for i in range(len(val_preds_proba)):\n",
    "            top_3_indices = np.argsort(val_preds_proba[i])[-3:][::-1]\n",
    "            val_ranked_labels.append([fertilizer_classes[idx] for idx in top_3_indices])\n",
    "\n",
    "        current_map3 = mapk([[label] for label in y_val_original_labels], val_ranked_labels, k=3)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val MAP@3: {current_map3:.5f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if current_map3 > best_map3:\n",
    "            best_map3 = current_map3\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model state\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch} as validation MAP@3 did not improve for {early_stopping_patience} epochs.\")\n",
    "                break\n",
    "    \n",
    "    # Load best model weights before returning\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return best_map3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f01e2855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 5-Fold Cross-Validation for TabTransformer...\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 training data shape (numerical part): (700000, 38)\n",
      "Fold 1 training data shape (categorical part): (700000, 9)\n",
      "Fold 1 validation data shape (numerical part): (150000, 38)\n",
      "Fold 1 validation data shape (categorical part): (150000, 9)\n",
      "Training on 700000 samples, validating on 150000 samples.\n",
      "Epoch 1/30, Train Loss: 1.9513, Val MAP@3: 0.30885\n",
      "Epoch 2/30, Train Loss: 1.9340, Val MAP@3: 0.31556\n",
      "Epoch 3/30, Train Loss: 1.9278, Val MAP@3: 0.32103\n",
      "Epoch 4/30, Train Loss: 1.9223, Val MAP@3: 0.32610\n",
      "Epoch 5/30, Train Loss: 1.9160, Val MAP@3: 0.33279\n",
      "Epoch 6/30, Train Loss: 1.9072, Val MAP@3: 0.33772\n",
      "Epoch 7/30, Train Loss: 1.8971, Val MAP@3: 0.34268\n",
      "Epoch 8/30, Train Loss: 1.8861, Val MAP@3: 0.34580\n",
      "Epoch 9/30, Train Loss: 1.8768, Val MAP@3: 0.34774\n",
      "Epoch 10/30, Train Loss: 1.8707, Val MAP@3: 0.34917\n",
      "Epoch 11/30, Train Loss: 1.8981, Val MAP@3: 0.34043\n",
      "Epoch 12/30, Train Loss: 1.8905, Val MAP@3: 0.34203\n",
      "Epoch 13/30, Train Loss: 1.8818, Val MAP@3: 0.34577\n",
      "Epoch 14/30, Train Loss: 1.8709, Val MAP@3: 0.34659\n",
      "Epoch 15/30, Train Loss: 1.8591, Val MAP@3: 0.34927\n",
      "Epoch 16/30, Train Loss: 1.8457, Val MAP@3: 0.35140\n",
      "Epoch 17/30, Train Loss: 1.8315, Val MAP@3: 0.35163\n",
      "Epoch 18/30, Train Loss: 1.8174, Val MAP@3: 0.35205\n",
      "Epoch 19/30, Train Loss: 1.8058, Val MAP@3: 0.35292\n",
      "Epoch 20/30, Train Loss: 1.7998, Val MAP@3: 0.35347\n",
      "Epoch 21/30, Train Loss: 1.8420, Val MAP@3: 0.34666\n",
      "Epoch 22/30, Train Loss: 1.8365, Val MAP@3: 0.34799\n",
      "Epoch 23/30, Train Loss: 1.8291, Val MAP@3: 0.34982\n",
      "Epoch 24/30, Train Loss: 1.8182, Val MAP@3: 0.35056\n",
      "Epoch 25/30, Train Loss: 1.8057, Val MAP@3: 0.34991\n",
      "Epoch 26/30, Train Loss: 1.7902, Val MAP@3: 0.35060\n",
      "Epoch 27/30, Train Loss: 1.7765, Val MAP@3: 0.35109\n",
      "Epoch 28/30, Train Loss: 1.7614, Val MAP@3: 0.35124\n",
      "Epoch 29/30, Train Loss: 1.7490, Val MAP@3: 0.35154\n",
      "Epoch 30/30, Train Loss: 1.7424, Val MAP@3: 0.35158\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 training data shape (numerical part): (700000, 38)\n",
      "Fold 2 training data shape (categorical part): (700000, 9)\n",
      "Fold 2 validation data shape (numerical part): (150000, 38)\n",
      "Fold 2 validation data shape (categorical part): (150000, 9)\n",
      "Training on 700000 samples, validating on 150000 samples.\n",
      "Epoch 1/30, Train Loss: 1.9522, Val MAP@3: 0.30547\n",
      "Epoch 2/30, Train Loss: 1.9342, Val MAP@3: 0.31445\n",
      "Epoch 3/30, Train Loss: 1.9281, Val MAP@3: 0.32026\n",
      "Epoch 4/30, Train Loss: 1.9227, Val MAP@3: 0.32546\n",
      "Epoch 5/30, Train Loss: 1.9162, Val MAP@3: 0.33063\n",
      "Epoch 6/30, Train Loss: 1.9077, Val MAP@3: 0.33638\n",
      "Epoch 7/30, Train Loss: 1.8978, Val MAP@3: 0.33978\n",
      "Epoch 8/30, Train Loss: 1.8871, Val MAP@3: 0.34530\n",
      "Epoch 9/30, Train Loss: 1.8780, Val MAP@3: 0.34796\n",
      "Epoch 10/30, Train Loss: 1.8720, Val MAP@3: 0.34875\n",
      "Epoch 11/30, Train Loss: 1.8986, Val MAP@3: 0.33967\n",
      "Epoch 12/30, Train Loss: 1.8915, Val MAP@3: 0.34296\n",
      "Epoch 13/30, Train Loss: 1.8828, Val MAP@3: 0.34509\n",
      "Epoch 14/30, Train Loss: 1.8727, Val MAP@3: 0.34703\n",
      "Epoch 15/30, Train Loss: 1.8605, Val MAP@3: 0.34845\n",
      "Epoch 16/30, Train Loss: 1.8477, Val MAP@3: 0.35118\n",
      "Epoch 17/30, Train Loss: 1.8331, Val MAP@3: 0.35181\n",
      "Epoch 18/30, Train Loss: 1.8193, Val MAP@3: 0.35227\n",
      "Epoch 19/30, Train Loss: 1.8082, Val MAP@3: 0.35277\n",
      "Epoch 20/30, Train Loss: 1.8014, Val MAP@3: 0.35328\n",
      "Epoch 21/30, Train Loss: 1.8436, Val MAP@3: 0.34801\n",
      "Epoch 22/30, Train Loss: 1.8382, Val MAP@3: 0.34895\n",
      "Epoch 23/30, Train Loss: 1.8301, Val MAP@3: 0.34907\n",
      "Epoch 24/30, Train Loss: 1.8197, Val MAP@3: 0.34999\n",
      "Epoch 25/30, Train Loss: 1.8066, Val MAP@3: 0.35072\n",
      "Epoch 26/30, Train Loss: 1.7934, Val MAP@3: 0.35055\n",
      "Epoch 27/30, Train Loss: 1.7778, Val MAP@3: 0.35137\n",
      "Epoch 28/30, Train Loss: 1.7623, Val MAP@3: 0.35174\n",
      "Epoch 29/30, Train Loss: 1.7511, Val MAP@3: 0.35150\n",
      "Epoch 30/30, Train Loss: 1.7440, Val MAP@3: 0.35160\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 training data shape (numerical part): (700000, 38)\n",
      "Fold 3 training data shape (categorical part): (700000, 9)\n",
      "Fold 3 validation data shape (numerical part): (150000, 38)\n",
      "Fold 3 validation data shape (categorical part): (150000, 9)\n",
      "Training on 700000 samples, validating on 150000 samples.\n",
      "Epoch 1/30, Train Loss: 1.9517, Val MAP@3: 0.30583\n",
      "Epoch 2/30, Train Loss: 1.9341, Val MAP@3: 0.31647\n",
      "Epoch 3/30, Train Loss: 1.9283, Val MAP@3: 0.32024\n",
      "Epoch 4/30, Train Loss: 1.9227, Val MAP@3: 0.32562\n",
      "Epoch 5/30, Train Loss: 1.9162, Val MAP@3: 0.33373\n",
      "Epoch 6/30, Train Loss: 1.9077, Val MAP@3: 0.33686\n",
      "Epoch 7/30, Train Loss: 1.8973, Val MAP@3: 0.34231\n",
      "Epoch 8/30, Train Loss: 1.8855, Val MAP@3: 0.34744\n",
      "Epoch 9/30, Train Loss: 1.8764, Val MAP@3: 0.34891\n",
      "Epoch 10/30, Train Loss: 1.8700, Val MAP@3: 0.35043\n",
      "Epoch 11/30, Train Loss: 1.8979, Val MAP@3: 0.34108\n",
      "Epoch 12/30, Train Loss: 1.8902, Val MAP@3: 0.34265\n",
      "Epoch 13/30, Train Loss: 1.8813, Val MAP@3: 0.34565\n",
      "Epoch 14/30, Train Loss: 1.8711, Val MAP@3: 0.34857\n",
      "Epoch 15/30, Train Loss: 1.8591, Val MAP@3: 0.35008\n",
      "Epoch 16/30, Train Loss: 1.8452, Val MAP@3: 0.35162\n",
      "Epoch 17/30, Train Loss: 1.8312, Val MAP@3: 0.35239\n",
      "Epoch 18/30, Train Loss: 1.8177, Val MAP@3: 0.35293\n",
      "Epoch 19/30, Train Loss: 1.8058, Val MAP@3: 0.35372\n",
      "Epoch 20/30, Train Loss: 1.7991, Val MAP@3: 0.35396\n",
      "Epoch 21/30, Train Loss: 1.8417, Val MAP@3: 0.34802\n",
      "Epoch 22/30, Train Loss: 1.8371, Val MAP@3: 0.34883\n",
      "Epoch 23/30, Train Loss: 1.8286, Val MAP@3: 0.34982\n",
      "Epoch 24/30, Train Loss: 1.8184, Val MAP@3: 0.34944\n",
      "Epoch 25/30, Train Loss: 1.8054, Val MAP@3: 0.35063\n",
      "Epoch 26/30, Train Loss: 1.7908, Val MAP@3: 0.35082\n",
      "Epoch 27/30, Train Loss: 1.7760, Val MAP@3: 0.35185\n",
      "Epoch 28/30, Train Loss: 1.7613, Val MAP@3: 0.35174\n",
      "Epoch 29/30, Train Loss: 1.7489, Val MAP@3: 0.35214\n",
      "Epoch 30/30, Train Loss: 1.7418, Val MAP@3: 0.35208\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 training data shape (numerical part): (700000, 38)\n",
      "Fold 4 training data shape (categorical part): (700000, 9)\n",
      "Fold 4 validation data shape (numerical part): (150000, 38)\n",
      "Fold 4 validation data shape (categorical part): (150000, 9)\n",
      "Training on 700000 samples, validating on 150000 samples.\n",
      "Epoch 1/30, Train Loss: 1.9510, Val MAP@3: 0.30586\n",
      "Epoch 2/30, Train Loss: 1.9328, Val MAP@3: 0.31512\n",
      "Epoch 3/30, Train Loss: 1.9268, Val MAP@3: 0.32380\n",
      "Epoch 4/30, Train Loss: 1.9211, Val MAP@3: 0.32764\n",
      "Epoch 5/30, Train Loss: 1.9145, Val MAP@3: 0.33202\n",
      "Epoch 6/30, Train Loss: 1.9065, Val MAP@3: 0.33611\n",
      "Epoch 7/30, Train Loss: 1.8972, Val MAP@3: 0.34024\n",
      "Epoch 8/30, Train Loss: 1.8871, Val MAP@3: 0.34419\n",
      "Epoch 9/30, Train Loss: 1.8777, Val MAP@3: 0.34667\n",
      "Epoch 10/30, Train Loss: 1.8717, Val MAP@3: 0.34715\n",
      "Epoch 11/30, Train Loss: 1.8990, Val MAP@3: 0.33833\n",
      "Epoch 12/30, Train Loss: 1.8914, Val MAP@3: 0.34088\n",
      "Epoch 13/30, Train Loss: 1.8830, Val MAP@3: 0.34470\n",
      "Epoch 14/30, Train Loss: 1.8716, Val MAP@3: 0.34691\n",
      "Epoch 15/30, Train Loss: 1.8606, Val MAP@3: 0.34806\n",
      "Epoch 16/30, Train Loss: 1.8470, Val MAP@3: 0.35010\n",
      "Epoch 17/30, Train Loss: 1.8327, Val MAP@3: 0.35058\n",
      "Epoch 18/30, Train Loss: 1.8187, Val MAP@3: 0.35150\n",
      "Epoch 19/30, Train Loss: 1.8074, Val MAP@3: 0.35248\n",
      "Epoch 20/30, Train Loss: 1.8003, Val MAP@3: 0.35282\n",
      "Epoch 21/30, Train Loss: 1.8437, Val MAP@3: 0.34750\n",
      "Epoch 22/30, Train Loss: 1.8379, Val MAP@3: 0.34783\n",
      "Epoch 23/30, Train Loss: 1.8297, Val MAP@3: 0.34969\n",
      "Epoch 24/30, Train Loss: 1.8195, Val MAP@3: 0.34986\n",
      "Epoch 25/30, Train Loss: 1.8072, Val MAP@3: 0.34930\n",
      "Epoch 26/30, Train Loss: 1.7928, Val MAP@3: 0.35034\n",
      "Epoch 27/30, Train Loss: 1.7768, Val MAP@3: 0.35097\n",
      "Epoch 28/30, Train Loss: 1.7618, Val MAP@3: 0.35101\n",
      "Epoch 29/30, Train Loss: 1.7509, Val MAP@3: 0.35111\n",
      "Epoch 30/30, Train Loss: 1.7421, Val MAP@3: 0.35113\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 training data shape (numerical part): (700000, 38)\n",
      "Fold 5 training data shape (categorical part): (700000, 9)\n",
      "Fold 5 validation data shape (numerical part): (150000, 38)\n",
      "Fold 5 validation data shape (categorical part): (150000, 9)\n",
      "Training on 700000 samples, validating on 150000 samples.\n",
      "Epoch 1/30, Train Loss: 1.9509, Val MAP@3: 0.30275\n",
      "Epoch 2/30, Train Loss: 1.9341, Val MAP@3: 0.31504\n",
      "Epoch 3/30, Train Loss: 1.9286, Val MAP@3: 0.32168\n",
      "Epoch 4/30, Train Loss: 1.9230, Val MAP@3: 0.32504\n",
      "Epoch 5/30, Train Loss: 1.9165, Val MAP@3: 0.33198\n",
      "Epoch 6/30, Train Loss: 1.9078, Val MAP@3: 0.33672\n",
      "Epoch 7/30, Train Loss: 1.8974, Val MAP@3: 0.34123\n",
      "Epoch 8/30, Train Loss: 1.8872, Val MAP@3: 0.34570\n",
      "Epoch 9/30, Train Loss: 1.8769, Val MAP@3: 0.34809\n",
      "Epoch 10/30, Train Loss: 1.8713, Val MAP@3: 0.34865\n",
      "Epoch 11/30, Train Loss: 1.8983, Val MAP@3: 0.34083\n",
      "Epoch 12/30, Train Loss: 1.8908, Val MAP@3: 0.34370\n",
      "Epoch 13/30, Train Loss: 1.8818, Val MAP@3: 0.34444\n",
      "Epoch 14/30, Train Loss: 1.8716, Val MAP@3: 0.34733\n",
      "Epoch 15/30, Train Loss: 1.8601, Val MAP@3: 0.35060\n",
      "Epoch 16/30, Train Loss: 1.8469, Val MAP@3: 0.35027\n",
      "Epoch 17/30, Train Loss: 1.8325, Val MAP@3: 0.35176\n",
      "Epoch 18/30, Train Loss: 1.8194, Val MAP@3: 0.35310\n",
      "Epoch 19/30, Train Loss: 1.8076, Val MAP@3: 0.35314\n",
      "Epoch 20/30, Train Loss: 1.8005, Val MAP@3: 0.35365\n",
      "Epoch 21/30, Train Loss: 1.8430, Val MAP@3: 0.34934\n",
      "Epoch 22/30, Train Loss: 1.8375, Val MAP@3: 0.34910\n",
      "Epoch 23/30, Train Loss: 1.8301, Val MAP@3: 0.34889\n",
      "Epoch 24/30, Train Loss: 1.8195, Val MAP@3: 0.34892\n",
      "Epoch 25/30, Train Loss: 1.8068, Val MAP@3: 0.35019\n",
      "Epoch 26/30, Train Loss: 1.7920, Val MAP@3: 0.35011\n",
      "Epoch 27/30, Train Loss: 1.7776, Val MAP@3: 0.35101\n",
      "Epoch 28/30, Train Loss: 1.7629, Val MAP@3: 0.35147\n",
      "Epoch 29/30, Train Loss: 1.7510, Val MAP@3: 0.35185\n",
      "Epoch 30/30, Train Loss: 1.7437, Val MAP@3: 0.35206\n",
      "\n",
      "Cross-validation complete.\n",
      "\n",
      "Individual Fold MAP@3 scores: ['0.35347', '0.35328', '0.35396', '0.35282', '0.35365']\n",
      "Average Fold MAP@3: 0.35343\n"
     ]
    }
   ],
   "source": [
    "# --- Main Cross-Validation Loop ---\n",
    "oof_preds_total = np.zeros((len(X_original_fe), NUM_CLASSES))\n",
    "test_preds_list_total = []\n",
    "fold_map3_scores_list = []\n",
    "\n",
    "print(f\"\\nStarting {configs.n_splits}-Fold Cross-Validation for TabTransformer...\")\n",
    "\n",
    "# Define StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=configs.n_splits, shuffle=True, random_state=configs.seed)\n",
    "\n",
    "for fold, (train_idx_original, val_idx_original) in enumerate(skf.split(X_original_fe, y_original_encoded)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{configs.n_splits} ---\")\n",
    "\n",
    "    # Use the final_numerical_features and final_categorical_features to select columns\n",
    "    X_train_fold_original_num = X_original_fe.iloc[train_idx_original][final_numerical_features].values \n",
    "    X_train_fold_original_cat = X_original_fe.iloc[train_idx_original][final_categorical_features].values\n",
    "    y_train_fold_original = y_original_encoded[train_idx_original]\n",
    "\n",
    "    X_val_fold_num = X_original_fe.iloc[val_idx_original][final_numerical_features].values\n",
    "    X_val_fold_cat = X_original_fe.iloc[val_idx_original][final_categorical_features].values\n",
    "    y_val_fold_original_labels = y_original.iloc[val_idx_original].values \n",
    "    y_val_fold_encoded = y_original_encoded[val_idx_original]\n",
    "\n",
    "    X_train_final_num = np.concatenate([X_train_fold_original_num, X_additional_fe[final_numerical_features].values])\n",
    "    X_train_final_cat = np.concatenate([X_train_fold_original_cat, X_additional_fe[final_categorical_features].values])\n",
    "    y_train_final = np.concatenate([y_train_fold_original, y_additional_encoded])\n",
    "\n",
    "    print(f\"Fold {fold+1} training data shape (numerical part): {X_train_final_num.shape}\")\n",
    "    print(f\"Fold {fold+1} training data shape (categorical part): {X_train_final_cat.shape}\")\n",
    "    print(f\"Fold {fold+1} validation data shape (numerical part): {X_val_fold_num.shape}\")\n",
    "    print(f\"Fold {fold+1} validation data shape (categorical part): {X_val_fold_cat.shape}\")\n",
    "\n",
    "    # Convert to PyTorch Tensors\n",
    "    train_num_tensor = torch.tensor(X_train_final_num, dtype=torch.float32)\n",
    "    train_cat_tensor = torch.tensor(X_train_final_cat, dtype=torch.long)\n",
    "    train_labels_tensor = torch.tensor(y_train_final, dtype=torch.long)\n",
    "\n",
    "    val_num_tensor = torch.tensor(X_val_fold_num, dtype=torch.float32)\n",
    "    val_cat_tensor = torch.tensor(X_val_fold_cat, dtype=torch.long)\n",
    "    val_labels_tensor = torch.tensor(y_val_fold_encoded, dtype=torch.long)\n",
    "    \n",
    "    test_num_tensor = torch.tensor(X_test_fe[final_numerical_features].values, dtype=torch.float32) \n",
    "    test_cat_tensor = torch.tensor(X_test_fe[final_categorical_features].values, dtype=torch.long)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(train_num_tensor, train_cat_tensor, train_labels_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=configs.batch_size, shuffle=True, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "    val_dataset = TensorDataset(val_num_tensor, val_cat_tensor, val_labels_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=configs.batch_size, shuffle=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "    test_dataset = TensorDataset(test_num_tensor, test_cat_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=configs.batch_size, shuffle=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "    # Initialize model, optimizer, loss\n",
    "    model = TabTransformer(\n",
    "        numerical_features_count=len(final_numerical_features),\n",
    "        categorical_dims_dict=categorical_dims,\n",
    "        embed_dim=MODEL_PARAMS['embed_dim'],\n",
    "        num_heads=MODEL_PARAMS['num_heads'],\n",
    "        num_transformer_layers=MODEL_PARAMS['num_transformer_layers'],\n",
    "        ff_hidden_dim=MODEL_PARAMS['ff_hidden_dim'],\n",
    "        dropout_rate=MODEL_PARAMS['dropout_rate'],\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=MODEL_PARAMS['learning_rate'], weight_decay=MODEL_PARAMS['weight_decay'])\n",
    "    \n",
    "    # --- Use class_weights_tensor if calculated, otherwise no weights ---\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    # Train and evaluate\n",
    "    fold_best_map3 = train_and_evaluate_model(\n",
    "        model, train_loader, val_loader, optimizer, criterion, \n",
    "        configs.max_expochs, DEVICE, configs.early_stopping_patience, fertilizer_classes, \n",
    "        y_val_fold_original_labels\n",
    "    )\n",
    "    fold_map3_scores_list.append(fold_best_map3)\n",
    "\n",
    "    model.eval()\n",
    "    fold_oof_preds_proba = []\n",
    "    with torch.no_grad():\n",
    "        for num_batch, cat_batch, _ in val_loader:\n",
    "            num_batch, cat_batch = num_batch.to(DEVICE), cat_batch.to(DEVICE)\n",
    "            outputs = model(num_batch, cat_batch)\n",
    "            fold_oof_preds_proba.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "    oof_preds_total[val_idx_original] = np.array(fold_oof_preds_proba)\n",
    "\n",
    "    fold_test_preds_proba = []\n",
    "    with torch.no_grad():\n",
    "        for num_batch, cat_batch in test_loader:\n",
    "            num_batch, cat_batch = num_batch.to(DEVICE), cat_batch.to(DEVICE)\n",
    "            outputs = model(num_batch, cat_batch)\n",
    "            fold_test_preds_proba.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "    test_preds_list_total.append(np.array(fold_test_preds_proba))\n",
    "    \n",
    "    del model, train_loader, val_loader, test_loader, train_num_tensor, train_cat_tensor, train_labels_tensor, val_num_tensor, val_cat_tensor, val_labels_tensor, test_num_tensor, test_cat_tensor\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"\\nCross-validation complete.\")\n",
    "print(\"\\nIndividual Fold MAP@3 scores:\", [f\"{s:.5f}\" for s in fold_map3_scores_list])\n",
    "print(f\"Average Fold MAP@3: {np.mean(fold_map3_scores_list):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e7b4720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating Overall OOF MAP@3 score...\n",
      "Overall OOF MAP@3: 0.35343\n"
     ]
    }
   ],
   "source": [
    "# --- Overall OOF MAP@3 Calculation (on OOF predictions) ---\n",
    "y_true_labels_for_map = [[label] for label in y_original.values]\n",
    "\n",
    "oof_ranked_labels = []\n",
    "for i in range(len(oof_preds_total)):\n",
    "    top_3_indices = np.argsort(oof_preds_total[i])[-3:][::-1]\n",
    "    oof_ranked_labels.append([fertilizer_classes[idx] for idx in top_3_indices])\n",
    "\n",
    "print(\"\\nCalculating Overall OOF MAP@3 score...\")\n",
    "oof_map3_score = mapk(y_true_labels_for_map, oof_ranked_labels, k=3)\n",
    "print(f\"Overall OOF MAP@3: {oof_map3_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9573bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id         Fertilizer Name\n",
      "0  750000         DAP 28-28 20-20\n",
      "1  750001  17-17-17 10-26-26 Urea\n",
      "2  750002          Urea DAP 20-20\n",
      "3  750003   14-35-14 DAP 10-26-26\n",
      "4  750004     20-20 Urea 10-26-26\n"
     ]
    }
   ],
   "source": [
    "# --- Generate Submission File ---\n",
    "final_test_preds = np.mean(test_preds_list_total, axis=0)\n",
    "\n",
    "test_ranked_labels = []\n",
    "for i in range(len(final_test_preds)):\n",
    "    top_3_indices = np.argsort(final_test_preds[i])[-3:][::-1]\n",
    "    top_3_fertilizers = [fertilizer_classes[idx] for idx in top_3_indices]\n",
    "    test_ranked_labels.append(\" \".join(top_3_fertilizers))\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Fertilizer Name': test_ranked_labels\n",
    "})\n",
    "\n",
    "submission_df.to_csv('TabTransform_submission_1.csv', index=False)\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea088f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
